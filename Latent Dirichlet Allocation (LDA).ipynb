{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is LDA?\n",
    "\n",
    "Latent Dirichlet allocation (LDA) is a topic model that generates topics based on word frequency from a set of documents. LDA is particularly useful for finding reasonably accurate mixtures of topics within a given document set.\n",
    "\n",
    "LDA walkthrough\n",
    "\n",
    "This walkthrough goes through the process of generating an LDA model with a highly simplified document set. This is not an exhaustive explanation of LDA. The goal of this walkthrough is to guide users through key steps in preparing their data and providing example output.\n",
    "\n",
    "Packages required\n",
    "\n",
    "This walkthrough uses the following Python packages:\n",
    "\n",
    "NLTK, a natural language toolkit for Python. A useful package for any natural language processing.\n",
    "\n",
    "For Mac/Unix with pip: $ sudo pip install -U nltk.\n",
    "\n",
    "stop_words, a Python package containing stop words.\n",
    "\n",
    "For Mac/Unix with pip: $ sudo pip install stop-words.\n",
    "\n",
    "gensim, a topic modeling package containing our LDA model.\n",
    "\n",
    "For Mac/Unix with pip: $ sudo pip install gensim.\n",
    "\n",
    "\n",
    "So what does LDA actually do?\n",
    "\n",
    "\n",
    "This explanation is a little lengthy, but useful for understanding the model we worked so hard to generate.\n",
    "\n",
    "LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution, like the ones in our walkthrough model. In other words, LDA assumes a document is made from the following steps:\n",
    "\n",
    "Determine the number of words in a document. Let’s say our document has 6 words.\n",
    "Determine the mixture of topics in that document. For example, the document might contain 1/2 the topic “health” and 1/2 the topic “vegetables.”\n",
    "Using each topic’s multinomial distribution, output words to fill the document’s word slots. In our example, the “health” topic is 1/2 our document, or 3 words. The “health” topic might have the word “diet” at 20% probability or “exercise” at 15%, so it will fill the document word slots based on those probabilities.\n",
    "Given this assumption of how documents are created, LDA backtracks and tries to figure out what topics would create those documents in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n"
     ]
    }
   ],
   "source": [
    "raw = doc_a.lower()\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "# create Engligh stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'likes', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "stopped_tokens=[i for i in tokens if not i in en_stop]\n",
    "print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "# stem token\n",
    "texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-fa458ca5a5e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[1;31m# update Dictionary with the document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         logger.info(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \"\"\"\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lihaaf', 'is', 'a', 'text', 'that', 'challenges', 'some', 'of', 'the', 'key', 'tenets', 'of', 'a', 'certain', 'kind', 'of', 'feminism', 'what', 'are', 'we', 'to', 'make', 'of', 'the', 'begum', 's', 'transformation', 'into', 'a', 'sexual', 'predator', 'are', 'we', 'to', 'see', 'her', 'de', 'formation', 'as', 'itself', 'a', 'response', 'to', 'her', 'patriarchy', 'domination', 'by', 'the', 'nawab', 'and', 'by', 'her', 'immediate', 'hyper', 'conservative', 'milieu', 'or', 'would', 'we', 'say', 'since', 'we', 'want', 'to', 'called', 'her', 'agential', 'that', 'she', 's', 'a', 'hero', 'and', 'villain', 'of', 'her', 'own', 'making', 'begum', 'jan', 'is', 'more', 'complex', 'than', 'a', 'simple', 'victim', 'of', 'patriarchy', 'women', 'back', 'in', '1900', 's', 'married', 'in', 'their', 'teen', 's', 'and', 'early', 'twenties', 'they', 'have', 'too', 'little', 'even', 'no', 'rights', 'and', 'largely', 'depend', 'on', 'their', 'fathers', 'husbands', 'or', 'sons', 'in', 'this', 'short', 'and', 'simple', 'yet', 'bravely', 'told', 'story', 'ismat', 'chughtai', 'address', 'an', 'unspeakable', 'issue', 'to', 'her', 'audience', 'deeply', 'religious', 'and', 'conservative', 'indians', 'begum', 'jan', 'is', 'trapped', 'in', 'a', 'disappointing', 'marriage', 'she', 'is', 'neglected', 'by', 'her', 'husband', 'nawab', 'begum', 's', 'frustration', 'and', 'the', 'events', 'that', 'took', 'around', 'her', 'the', 'author', 'captured', 'them', 'in', 'a', 'sympathetic', 'almost', 'comical', 'manner', 'through', 'the', 'eyes', 'of', 'a', 'child', 'that', 'does', 'not', 'completely', 'understand', 'what', 'she', 's', 'witnessing', 'begum', 'surrounded', 'by', 'attendees', 'supplies', 'and', 'luxurious', 'is', 'lonely', 'and', 'yearn', 'for', 'great', 'love', 'and', 'attentions', 'the', 'funny', 'things', 'the', 'child', 'witnessed', 'of', 'begum', 'and', 'rabbu', 's', 'relationship', 'traumatised', 'the', 'child', 'left', 'alone', 'by', 'her', 'husband', 'she', 'takes', 'charge', 'of', 'her', 'life', 'and', 'navigates', 'her', 'way', 'through', 'the', 'binding', 'of', 'the', 'patriarchal', 'setup', 'to', 'express', 'her', 'sexual', 'urges', 'and', 'satiate', 'them', 'she', 'might', 'be', 'secluded', 'in', 'her', 'husband', 's', 'household', 'but', 'she', 'used', 'the', 'imposed', 'seclusion', 'to', 'her', 'advantage', 'she', 'created', 'a', 'world', 'for', 'herself', 'once', 'in', 'there', 'she', 'is', 'no', 'longer', 'at', 'the', 'mercy', 'of', 'her', 'husband', 'she', 'can', 'unhesitatingly', 'voice', 'an', 'itch', 'on', 'her', 'entire', 'existence', 'revolved', 'and', 'find', 'the', 'necessary', 'means', 'in', 'rabbu', 'to', 'tend', 'it', 'and', 'she', 'does', 'would', 'the', 'begum', 'has', 'turned', 'to', 'rabbu', 'if', 'her', 'husband', 'has', 'been', 'more', 'attentive', 'to', 'her', 'needs', 'or', 'perhaps', 'if', 'her', 'situation', 'was', 'despite', 'knowing', 'from', 'a', 'young', 'age', 'she', 'was', 'gay', 'she', 'might', 'knew', 'telling', 'the', 'parents', 'would', 'cause', 'a', 'rift', 'that', 'might', 'put', 'insurmountable', 'even', 'if', 'she', 'was', 'in', 'a', 'little', 'bit', 'more', 'of', 'modern', 'and', 'understanding', 'family', 'most', 'psychologists', 'in', 'the', '19th', 'and', '20th', 'centuries', 'classified', 'homosexuality', 'as', 'a', 'form', 'of', 'mental', 'illness', 'many', 'were', 'subjected', 'to', 'psychiatric', 'treatment', 'with', 'the', 'aim', 'to', 'cure', 'their', 'homosexuality', 'and', 'eventually', 'she', 'had', 'to', 'face', 'it', 'however', 'that', 'was', 'not', 'in', 'her', 'case', 'even', 'if', 'begum', 'come', 'clean', 'before', 'her', 'parents', 'before', 'her', 'societies', 'it', 'would', 'be', 'impossible', 'in', 'islam', 'as', 'in', 'many', 'christian', 'orthodox', 'denominations', 'as', 'in', 'orthodox', 'judaism', 'homosexuality', 'is', 'seen', 'as', 'a', 'sin']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#create english stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "#create p_stremmer of class Porterstemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "# create sample documents\n",
    "# doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "# doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "# doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "# doc_d = \"‘Lihaaf’ is a text that challenges some of the key tenets of a certain kind of feminism.What are we to make of the Begum’s transformation into a sexual predator? Are we to see her de-formation as itself a response to her patriarchy domination by the Nawab and by her immediate hyper conservative milieu? Or would we say, since we want to called her agential that she’s a hero and villain of her own making.Begum Jan is more complex than a simple victim of patriarchy. Women back in 1900’s married in their teen’s and early twenties. They have too little, even no rights, and largely depend on their fathers, husbands or sons. In this short and simple yet bravely told story, Ismat Chughtai address an unspeakable issue to her audience – deeply religious and conservative Indians. .\"\n",
    "# doc_e = \"Begum Jan is trapped in a disappointing marriage. She is neglected by her husband Nawab. Begum’s frustration and the events that took around her, the author captured them in a sympathetic, almost comical manner through the eyes of a child that does not completely understand what she’s witnessing. Begum, surrounded by attendees, supplies and luxurious, is lonely and yearn for great love and attentions. The funny things, the child witnessed of Begum and Rabbu’s relationship traumatised the child. Left alone by her husband, she takes charge of her life and navigates her way through the binding of the patriarchal setup to express her sexual urges and satiate them. She might be secluded in her husband’s household but she used the imposed seclusion to her advantage, she created a world for herself. Once in there, she is no longer at the mercy of her husband, she can unhesitatingly voice an ‘itch’ – on her entire existence revolved – and find the necessary means in Rabbu to tend it. And she does. Would the Begum has turned to Rabbu if her husband has been more attentive to her needs? Or perhaps if her situation was- despite knowing from a young age she was gay- she might knew telling the parents would cause a rift that might put insurmountable. Even if she was in a little bit more of modern and understanding family, most psychologists in the 19th and 20th centuries classified homosexuality as a form of mental illness. Many were subjected to psychiatric ‘treatment’ with the aim to cure their homosexuality. And eventually she had to face it. However that was not in her case, even if Begum ‘come clean’ before her parents, before her societies, it would be impossible. In Islam, as in many Christian orthodox denominations as in Orthodox Judaism, homosexuality is seen as a sin. .\" \n",
    "# doc_f = \"Most of all, for a woman ignored and victimized by patriarchy, and self-empowered, Begum Jan’s behaviour towards the protagonist in absence of Rabbu however was questionable. Reading the text merely as a feminist text has also led to our misplaced identification of who is the feminist in Chughtai’s story. The child narrator who can think of an egalitarian, open relationship with her brothers and common male friends and who even at her most terrified, gathers courage and speaks up. Resulting her mother to send her to Begum Jan and the zenana, that was supposed to empower her, punished her instead- silencing and pacifying her. No wonder it led to a great controversy back then, when Ismat Chughtai boldly bashing the social orders of deeply conservative societies where upbringing of a person was actually based on deep rooted conditionings.\"\n",
    "#compile sample document into list\n",
    "doc_a= \"\"\"Lihaaf’ is a text that challenges some of the key tenets of a certain kind of feminism. \n",
    "What are we to make of the Begum’s transformation into a sexual predator? Are we to see her ‘’de-formation’’ as itself a response to her patriarchy domination by the Nawab and by her immediate hyper conservative milieu? Or would we say, since we want to called her agential that she’s a hero and villain of her own making. \n",
    "Begum Jan is more complex than a simple victim of patriarchy. \n",
    "Women back in 1900’s married in their teen’s and early twenties. They have too little, even no rights, and largely depend on their fathers, husbands or sons. \n",
    "In this short and simple yet bravely told story, Ismat Chughtai address an unspeakable issue to her audience – deeply religious and conservative Indians. \n",
    "Begum Jan is trapped in a disappointing marriage. She is neglected by her husband Nawab. Begum’s frustration and the events that took around her, the author captured them in a sympathetic, almost comical manner through the eyes of a child that does not completely understand what she’s witnessing. Begum, surrounded by attendees, supplies and luxurious, is lonely and yearn for great love and attentions. The funny things, the child witnessed of Begum and Rabbu’s relationship traumatised the child. \n",
    "Left alone by her husband, she takes charge of her life and navigates her way through the binding of the patriarchal setup to express her sexual urges and satiate them. She might be secluded in her husband’s household but she used the imposed seclusion to her advantage, she created a world for herself. Once in there, she is no longer at the mercy of her husband, she can unhesitatingly voice an ‘itch’ – on her entire existence revolved – and find the necessary means in Rabbu to tend it. And she does. \n",
    "Would the Begum has turned to Rabbu if her husband has been more attentive to her needs? \n",
    "Or perhaps if her situation was- despite knowing from a young age she was gay- she might knew telling the parents would cause a rift that might put insurmountable. Even if she was in a little bit more of modern and understanding family, most psychologists in the 19th and 20th centuries classified homosexuality as a form of mental illness. Many were subjected to psychiatric ‘treatment’ with the aim to cure their homosexuality. And eventually she had to face it. However that was not in her case, even if Begum ‘come clean’ before her parents, before her societies, it would be impossible. \n",
    "In Islam, as in many Christian orthodox denominations as in Orthodox Judaism, homosexuality is seen as a sin.\"\"\"\n",
    " \n",
    "#doc_set = [doc_a,doc_b,doc_c,doc_d,doc_e, doc_f]\n",
    "doc_set = [doc_a]\n",
    "#list for tokenized document in loop\n",
    "texts=[]\n",
    "\n",
    "for i in doc_set:\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    print(tokens)\n",
    "    # remove stop words from tokens\n",
    "    stoped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    # stem token\n",
    "    stemmed_tokens= [ p_stemmer.stem(i) for i in stoped_tokens]\n",
    "    \n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "    # turn our tokenized document into a id ---> term dictonary\n",
    "    \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    # print(dictionary)\n",
    "    \n",
    "    #convert tokenised document into a document-term matrix\n",
    "    \n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "ldamodel =  gensim.models.ldamodel.LdaModel(corpus,num_topics = 3,id2word = dictionary,passes = 20)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '0.006*\"begum\" + 0.006*\"s\" + 0.006*\"husband\" + 0.006*\"rabbu\"')]\n"
     ]
    }
   ],
   "source": [
    " print(ldamodel.print_topics(num_topics=1, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method II\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/MikeEd/Downloads/Compressed/abcnews-date-text.csv\", error_bad_lines= False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index']= data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peek of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186018\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MikeEd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize and stem preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selcet a document to preview after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words on the Data set\n",
    "\n",
    "create a dictionary from 'processed_docs' containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dictionary in module gensim.corpora.dictionary object:\n",
      "\n",
      "class Dictionary(gensim.utils.SaveLoad, collections.abc.Mapping)\n",
      " |  Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
      " |  \n",
      " |  Notable instance attributes:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  token2id : dict of (str, int)\n",
      " |      token -> tokenId.\n",
      " |  id2token : dict of (int, str)\n",
      " |      Reverse mapping for token2id, initialized in a lazy manner to save memory (not created until needed).\n",
      " |  cfs : dict of (int, int)\n",
      " |      Collection frequencies: token_id -> how many instances of this token are contained in the documents.\n",
      " |  dfs : dict of (int, int)\n",
      " |      Document frequencies: token_id -> how many documents contain this token.\n",
      " |  num_docs : int\n",
      " |      Number of documents processed.\n",
      " |  num_pos : int\n",
      " |      Total number of corpus positions (number of processed words).\n",
      " |  num_nnz : int\n",
      " |      Total number of non-zeroes in the BOW matrix (sum of the number of unique\n",
      " |      words per document over the entire corpus).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dictionary\n",
      " |      gensim.utils.SaveLoad\n",
      " |      collections.abc.Mapping\n",
      " |      collections.abc.Collection\n",
      " |      collections.abc.Sized\n",
      " |      collections.abc.Iterable\n",
      " |      collections.abc.Container\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, tokenid)\n",
      " |      Get the string token that corresponds to `tokenid`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tokenid : int\n",
      " |          Id of token.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Token corresponding to `tokenid`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If this Dictionary doesn't contain such `tokenid`.\n",
      " |  \n",
      " |  __init__(self, documents=None, prune_at=2000000)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str, optional\n",
      " |          Documents to be used to initialize the mapping and collect corpus statistics.\n",
      " |      prune_at : int, optional\n",
      " |          Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n",
      " |          footprint, the correctness is not guaranteed.\n",
      " |          Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> texts = [['human', 'interface', 'computer']]\n",
      " |          >>> dct = Dictionary(texts)  # initialize a Dictionary\n",
      " |          >>> dct.add_documents([[\"cat\", \"say\", \"meow\"], [\"dog\"]])  # add more document (extend the vocabulary)\n",
      " |          >>> dct.doc2bow([\"dog\", \"computer\", \"non_existent_word\"])\n",
      " |          [(0, 1), (6, 1)]\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate over all tokens.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Get number of stored tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Number of stored tokens.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_documents(self, documents, prune_at=2000000)\n",
      " |      Update dictionary from a collection of `documents`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str\n",
      " |          Input corpus. All tokens should be already **tokenized and normalized**.\n",
      " |      prune_at : int, optional\n",
      " |          Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n",
      " |          footprint, the correctness is not guaranteed.\n",
      " |          Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [\"máma mele maso\".split(), \"ema má máma\".split()]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\n",
      " |          >>> len(dct)\n",
      " |          10\n",
      " |  \n",
      " |  compactify(self)\n",
      " |      Assign new word ids to all words, shrinking any gaps.\n",
      " |  \n",
      " |  doc2bow(self, document, allow_update=False, return_missing=False)\n",
      " |      Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document : list of str\n",
      " |          Input document.\n",
      " |      allow_update : bool, optional\n",
      " |          Update self, by adding new tokens from `document` and updating internal corpus statistics.\n",
      " |      return_missing : bool, optional\n",
      " |          Return missing tokens (tokens present in `document` but not in self) with frequencies?\n",
      " |      \n",
      " |      Return\n",
      " |      ------\n",
      " |      list of (int, int)\n",
      " |          BoW representation of `document`.\n",
      " |      list of (int, int), dict of (str, int)\n",
      " |          If `return_missing` is True, return BoW representation of `document` + dictionary with missing\n",
      " |          tokens and their frequencies.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> dct = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
      " |          >>> dct.doc2bow([\"this\", \"is\", \"máma\"])\n",
      " |          [(2, 1)]\n",
      " |          >>> dct.doc2bow([\"this\", \"is\", \"máma\"], return_missing=True)\n",
      " |          ([(2, 1)], {u'this': 1, u'is': 1})\n",
      " |  \n",
      " |  doc2idx(self, document, unknown_word_index=-1)\n",
      " |      Convert `document` (a list of words) into a list of indexes = list of `token_id`.\n",
      " |      Replace all unknown words i.e, words not in the dictionary with the index as set via `unknown_word_index`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document : list of str\n",
      " |          Input document\n",
      " |      unknown_word_index : int, optional\n",
      " |          Index to use for words not in the dictionary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of int\n",
      " |          Token ids for tokens in `document`, in the same order.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"a\", \"a\", \"b\"], [\"a\", \"c\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.doc2idx([\"a\", \"a\", \"c\", \"not_in_dictionary\", \"c\"])\n",
      " |          [0, 0, 2, -1, 2]\n",
      " |  \n",
      " |  filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000, keep_tokens=None)\n",
      " |      Filter out tokens in the dictionary by their frequency.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      no_below : int, optional\n",
      " |          Keep tokens which are contained in at least `no_below` documents.\n",
      " |      no_above : float, optional\n",
      " |          Keep tokens which are contained in no more than `no_above` documents\n",
      " |          (fraction of total corpus size, not an absolute number).\n",
      " |      keep_n : int, optional\n",
      " |          Keep only the first `keep_n` most frequent tokens.\n",
      " |      keep_tokens : iterable of str\n",
      " |          Iterable of tokens that **must** stay in dictionary after filtering.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This removes all tokens in the dictionary that are:\n",
      " |      \n",
      " |      #. Less frequent than `no_below` documents (absolute number, e.g. `5`) or \n",
      " |      \n",
      " |      #. More frequent than `no_above` documents (fraction of the total corpus size, e.g. `0.3`).\n",
      " |      #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `keep_n=None`).\n",
      " |      \n",
      " |      After the pruning, resulting gaps in word ids are shrunk.\n",
      " |      Due to this gap shrinking, **the same word may have a different word id before and after the call\n",
      " |      to this function!**\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.filter_extremes(no_below=1, no_above=0.5, keep_n=1)\n",
      " |          >>> len(dct)\n",
      " |          1\n",
      " |  \n",
      " |  filter_n_most_frequent(self, remove_n)\n",
      " |      Filter out the 'remove_n' most frequent tokens that appear in the documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      remove_n : int\n",
      " |          Number of the most frequent tokens that will be removed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.filter_n_most_frequent(2)\n",
      " |          >>> len(dct)\n",
      " |          3\n",
      " |  \n",
      " |  filter_tokens(self, bad_ids=None, good_ids=None)\n",
      " |      Remove the selected `bad_ids` tokens from :class:`~gensim.corpora.dictionary.Dictionary`.\n",
      " |      \n",
      " |      Alternatively, keep selected `good_ids` in :class:`~gensim.corpora.dictionary.Dictionary` and remove the rest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bad_ids : iterable of int, optional\n",
      " |          Collection of word ids to be removed.\n",
      " |      good_ids : collection of int, optional\n",
      " |          Keep selected collection of word ids and remove the rest.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> 'ema' in dct.token2id\n",
      " |          True\n",
      " |          >>> dct.filter_tokens(bad_ids=[dct.token2id['ema']])\n",
      " |          >>> 'ema' in dct.token2id\n",
      " |          False\n",
      " |          >>> len(dct)\n",
      " |          4\n",
      " |          >>> dct.filter_tokens(good_ids=[dct.token2id['maso']])\n",
      " |          >>> len(dct)\n",
      " |          1\n",
      " |  \n",
      " |  iteritems(self)\n",
      " |  \n",
      " |  iterkeys = __iter__(self)\n",
      " |  \n",
      " |  itervalues(self)\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Get all stored ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of int\n",
      " |          List of all token ids.\n",
      " |  \n",
      " |  merge_with(self, other)\n",
      " |      Merge another dictionary into this dictionary, mapping the same tokens to the same ids\n",
      " |      and new tokens to new ids.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The purpose is to merge two corpora created using two different dictionaries: `self` and `other`.\n",
      " |      `other` can be any id=>word mapping (a dict, a Dictionary object, ...).\n",
      " |      \n",
      " |      Return a transformation object which, when accessed as `result[doc_from_other_corpus]`, will convert documents\n",
      " |      from a corpus built using the `other` dictionary into a document using the new, merged dictionary.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : {dict, :class:`~gensim.corpora.dictionary.Dictionary`}\n",
      " |          Other dictionary.\n",
      " |      \n",
      " |      Return\n",
      " |      ------\n",
      " |      :class:`gensim.models.VocabTransform`\n",
      " |          Transformation object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus_1, corpus_2 = [[\"a\", \"b\", \"c\"]], [[\"a\", \"f\", \"f\"]]\n",
      " |          >>> dct_1, dct_2 = Dictionary(corpus_1), Dictionary(corpus_2)\n",
      " |          >>> dct_1.doc2bow(corpus_2[0])\n",
      " |          [(0, 1)]\n",
      " |          >>> transformer = dct_1.merge_with(dct_2)\n",
      " |          >>> dct_1.doc2bow(corpus_2[0])\n",
      " |          [(0, 1), (3, 2)]\n",
      " |  \n",
      " |  patch_with_special_tokens(self, special_token_dict)\n",
      " |      Patch token2id and id2token using a dictionary of special tokens.\n",
      " |      \n",
      " |      \n",
      " |      **Usecase:** when doing sequence modeling (e.g. named entity recognition), one may  want to specify\n",
      " |      special tokens that behave differently than others.\n",
      " |      One example is the \"unknown\" token, and another is the padding token.\n",
      " |      It is usual to set the padding token to have index `0`, and patching the dictionary with `{'<PAD>': 0}`\n",
      " |      would be one way to specify this.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      special_token_dict : dict of (str, int)\n",
      " |          dict containing the special tokens as keys and their wanted indices as values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>>\n",
      " |          >>> special_tokens = {'pad': 0, 'space': 1}\n",
      " |          >>> print(dct.token2id)\n",
      " |          {'maso': 0, 'mele': 1, 'máma': 2, 'ema': 3, 'má': 4}\n",
      " |          >>>\n",
      " |          >>> dct.patch_with_special_tokens(special_tokens)\n",
      " |          >>> print(dct.token2id)\n",
      " |          {'maso': 6, 'mele': 7, 'máma': 2, 'ema': 3, 'má': 4, 'pad': 0, 'space': 1}\n",
      " |  \n",
      " |  save_as_text(self, fname, sort_by_word=True)\n",
      " |      Save :class:`~gensim.corpora.dictionary.Dictionary` to a text file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to output file.\n",
      " |      sort_by_word : bool, optional\n",
      " |          Sort words in lexicographical order before writing them out?\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Format::\n",
      " |      \n",
      " |          num_docs\n",
      " |          id_1[TAB]word_1[TAB]document_frequency_1[NEWLINE]\n",
      " |          id_2[TAB]word_2[TAB]document_frequency_2[NEWLINE]\n",
      " |          ....\n",
      " |          id_k[TAB]word_k[TAB]document_frequency_k[NEWLINE]\n",
      " |      \n",
      " |      This text format is great for corpus inspection and debugging. As plaintext, it's also easily portable\n",
      " |      to other tools and frameworks. For better performance and to store the entire object state,\n",
      " |      including collected corpus statistics, use :meth:`~gensim.corpora.dictionary.Dictionary.save` and\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.load` instead.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.load_from_text`\n",
      " |          Load :class:`~gensim.corpora.dictionary.Dictionary` from text file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> from gensim.test.utils import get_tmpfile\n",
      " |          >>>\n",
      " |          >>> tmp_fname = get_tmpfile(\"dictionary\")\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>>\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.save_as_text(tmp_fname)\n",
      " |          >>>\n",
      " |          >>> loaded_dct = Dictionary.load_from_text(tmp_fname)\n",
      " |          >>> assert dct.token2id == loaded_dct.token2id\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_corpus(corpus, id2word=None)\n",
      " |      Create :class:`~gensim.corpora.dictionary.Dictionary` from an existing corpus.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of iterable of (int, number)\n",
      " |          Corpus in BoW format.\n",
      " |      id2word : dict of (int, object)\n",
      " |          Mapping id -> word. If None, the mapping `id2word[word_id] = str(word_id)` will be used.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This can be useful if you only have a term-document BOW matrix (represented by `corpus`), but not the original\n",
      " |      text corpus. This method will scan the term-document count matrix for all word ids that appear in it,\n",
      " |      then construct :class:`~gensim.corpora.dictionary.Dictionary` which maps each `word_id -> id2word[word_id]`.\n",
      " |      `id2word` is an optional dictionary that maps the `word_id` to a token.\n",
      " |      In case `id2word` isn't specified the mapping `id2word[word_id] = str(word_id)` will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          Inferred dictionary from corpus.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[(1, 1.0)], [], [(0, 5.0), (2, 1.0)], []]\n",
      " |          >>> dct = Dictionary.from_corpus(corpus)\n",
      " |          >>> len(dct)\n",
      " |          3\n",
      " |  \n",
      " |  from_documents(documents)\n",
      " |      Create :class:`~gensim.corpora.dictionary.Dictionary` from `documents`.\n",
      " |      \n",
      " |      Equivalent to `Dictionary(documents=documents)`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str\n",
      " |          Input corpus.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          Dictionary initialized from `documents`.\n",
      " |  \n",
      " |  load_from_text(fname)\n",
      " |      Load a previously stored :class:`~gensim.corpora.dictionary.Dictionary` from a text file.\n",
      " |      \n",
      " |      Mirror function to :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname: str\n",
      " |          Path to a file produced by :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`\n",
      " |          Save :class:`~gensim.corpora.dictionary.Dictionary` to text file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> from gensim.test.utils import get_tmpfile\n",
      " |          >>>\n",
      " |          >>> tmp_fname = get_tmpfile(\"dictionary\")\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>>\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.save_as_text(tmp_fname)\n",
      " |          >>>\n",
      " |          >>> loaded_dct = Dictionary.load_from_text(tmp_fname)\n",
      " |          >>> assert dct.token2id == loaded_dct.token2id\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      " |      Save the object to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      " |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      " |          loading and sharing the large arrays in RAM between multiple processes.\n",
      " |      \n",
      " |          If list of str: store these attributes into separate files. The automated size check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int, optional\n",
      " |          Don't store arrays smaller than this separately. In bytes.\n",
      " |      ignore : frozenset of str, optional\n",
      " |          Attributes that shouldn't be stored at all.\n",
      " |      pickle_protocol : int, optional\n",
      " |          Protocol number for pickle.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |          Load object from file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(fname, mmap=None) from abc.ABCMeta\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  __contains__(self, key)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  get(self, key, default=None)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(self)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  values(self)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __reversed__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Collection:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.corpora.Dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count = count +1\n",
    "    if count >10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words on the Data set\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(162, 1), (240, 1), (292, 1), (589, 1), (838, 1), (3567, 1), (3568, 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 162 (\"govt\") appears 1 time.\n",
      "Word 240 (\"group\") appears 1 time.\n",
      "Word 292 (\"vote\") appears 1 time.\n",
      "Word 589 (\"local\") appears 1 time.\n",
      "Word 838 (\"want\") appears 1 time.\n",
      "Word 3567 (\"compulsori\") appears 1 time.\n",
      "Word 3568 (\"ratepay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5850076620505259),\n",
      " (1, 0.38947256567331934),\n",
      " (2, 0.4997099083387053),\n",
      " (3, 0.5063271308533074)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running LDA using Bag of Words\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus,num_topics=5,id2word = dictionary, passes=2 , workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.009*\"health\" + 0.008*\"brisban\" + 0.008*\"rural\" + 0.007*\"tasmanian\" + 0.007*\"tasmania\" + 0.007*\"nation\" + 0.007*\"warn\" + 0.006*\"report\" + 0.006*\"communiti\" + 0.006*\"sydney\"\n",
      "Topic: 1 \n",
      "Words: 0.015*\"queensland\" + 0.012*\"market\" + 0.012*\"donald\" + 0.011*\"news\" + 0.009*\"coast\" + 0.009*\"dead\" + 0.008*\"miss\" + 0.008*\"south\" + 0.008*\"bushfir\" + 0.007*\"rise\"\n",
      "Topic: 2 \n",
      "Words: 0.030*\"australia\" + 0.024*\"australian\" + 0.016*\"elect\" + 0.014*\"world\" + 0.009*\"test\" + 0.008*\"final\" + 0.007*\"open\" + 0.006*\"farm\" + 0.006*\"win\" + 0.005*\"year\"\n",
      "Topic: 3 \n",
      "Words: 0.026*\"polic\" + 0.015*\"charg\" + 0.013*\"court\" + 0.013*\"death\" + 0.012*\"murder\" + 0.010*\"crash\" + 0.010*\"woman\" + 0.009*\"face\" + 0.009*\"die\" + 0.009*\"alleg\"\n",
      "Topic: 4 \n",
      "Words: 0.019*\"trump\" + 0.013*\"say\" + 0.012*\"govern\" + 0.010*\"chang\" + 0.008*\"school\" + 0.008*\"live\" + 0.006*\"countri\" + 0.006*\"fund\" + 0.006*\"plan\" + 0.006*\"power\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.010*\"govern\" + 0.008*\"monday\" + 0.007*\"michael\" + 0.007*\"royal\" + 0.007*\"david\" + 0.006*\"commiss\" + 0.006*\"sport\" + 0.006*\"budget\" + 0.005*\"histori\" + 0.005*\"abbott\"\n",
      "Topic: 1 Word: 0.019*\"live\" + 0.008*\"cattl\" + 0.008*\"week\" + 0.008*\"search\" + 0.007*\"septemb\" + 0.007*\"miss\" + 0.007*\"june\" + 0.006*\"plane\" + 0.006*\"cancer\" + 0.006*\"anim\"\n",
      "Topic: 2 Word: 0.026*\"trump\" + 0.020*\"news\" + 0.017*\"market\" + 0.016*\"rural\" + 0.008*\"share\" + 0.007*\"nation\" + 0.007*\"price\" + 0.007*\"rise\" + 0.007*\"dollar\" + 0.007*\"australian\"\n",
      "Topic: 3 Word: 0.010*\"hobart\" + 0.009*\"scott\" + 0.008*\"sexual\" + 0.008*\"street\" + 0.007*\"island\" + 0.006*\"tree\" + 0.006*\"islam\" + 0.006*\"right\" + 0.006*\"human\" + 0.006*\"music\"\n",
      "Topic: 4 Word: 0.015*\"interview\" + 0.008*\"morrison\" + 0.006*\"novemb\" + 0.006*\"marriag\" + 0.006*\"syria\" + 0.006*\"extend\" + 0.006*\"kill\" + 0.005*\"facebook\" + 0.005*\"hong\" + 0.005*\"kong\"\n",
      "Topic: 5 Word: 0.018*\"south\" + 0.015*\"north\" + 0.011*\"west\" + 0.010*\"coast\" + 0.009*\"queensland\" + 0.009*\"weather\" + 0.009*\"australia\" + 0.008*\"bushfir\" + 0.007*\"korea\" + 0.007*\"storm\"\n",
      "Topic: 6 Word: 0.017*\"charg\" + 0.017*\"polic\" + 0.016*\"murder\" + 0.012*\"woman\" + 0.011*\"court\" + 0.010*\"jail\" + 0.010*\"death\" + 0.010*\"crash\" + 0.010*\"alleg\" + 0.009*\"shoot\"\n",
      "Topic: 7 Word: 0.017*\"countri\" + 0.013*\"donald\" + 0.012*\"hour\" + 0.009*\"stori\" + 0.007*\"wednesday\" + 0.007*\"care\" + 0.006*\"climat\" + 0.006*\"chang\" + 0.005*\"farmer\" + 0.005*\"water\"\n",
      "Topic: 8 Word: 0.014*\"elect\" + 0.008*\"labor\" + 0.007*\"liber\" + 0.006*\"parti\" + 0.006*\"say\" + 0.006*\"senat\" + 0.006*\"govern\" + 0.006*\"andrew\" + 0.005*\"christma\" + 0.005*\"jam\"\n",
      "Topic: 9 Word: 0.013*\"drum\" + 0.012*\"world\" + 0.011*\"final\" + 0.009*\"leagu\" + 0.008*\"tuesday\" + 0.008*\"friday\" + 0.008*\"cricket\" + 0.007*\"australia\" + 0.007*\"grandstand\" + 0.006*\"open\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))Classification of the topics\n",
    "Performance evaluation by classifying sample document using LDA Bag of Words mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification of the topics\n",
    "Performance evaluation by classifying sample document using LDA Bag of Words mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6430763602256775\t \n",
      "Topic: 0.019*\"trump\" + 0.013*\"say\" + 0.012*\"govern\" + 0.010*\"chang\" + 0.008*\"school\" + 0.008*\"live\" + 0.006*\"countri\" + 0.006*\"fund\" + 0.006*\"plan\" + 0.006*\"power\"\n",
      "\n",
      "Score: 0.15764841437339783\t \n",
      "Topic: 0.030*\"australia\" + 0.024*\"australian\" + 0.016*\"elect\" + 0.014*\"world\" + 0.009*\"test\" + 0.008*\"final\" + 0.007*\"open\" + 0.006*\"farm\" + 0.006*\"win\" + 0.005*\"year\"\n",
      "\n",
      "Score: 0.14912059903144836\t \n",
      "Topic: 0.009*\"health\" + 0.008*\"brisban\" + 0.008*\"rural\" + 0.007*\"tasmanian\" + 0.007*\"tasmania\" + 0.007*\"nation\" + 0.007*\"warn\" + 0.006*\"report\" + 0.006*\"communiti\" + 0.006*\"sydney\"\n",
      "\n",
      "Score: 0.025121551007032394\t \n",
      "Topic: 0.015*\"queensland\" + 0.012*\"market\" + 0.012*\"donald\" + 0.011*\"news\" + 0.009*\"coast\" + 0.009*\"dead\" + 0.008*\"miss\" + 0.008*\"south\" + 0.008*\"bushfir\" + 0.007*\"rise\"\n",
      "\n",
      "Score: 0.025033066049218178\t \n",
      "Topic: 0.026*\"polic\" + 0.015*\"charg\" + 0.013*\"court\" + 0.013*\"death\" + 0.012*\"murder\" + 0.010*\"crash\" + 0.010*\"woman\" + 0.009*\"face\" + 0.009*\"die\" + 0.009*\"alleg\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \n",
    "Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8874712586402893\t \n",
      "Topic: 0.010*\"govern\" + 0.008*\"monday\" + 0.007*\"michael\" + 0.007*\"royal\" + 0.007*\"david\" + 0.006*\"commiss\" + 0.006*\"sport\" + 0.006*\"budget\" + 0.005*\"histori\" + 0.005*\"abbott\"\n",
      "\n",
      "Score: 0.012506821192800999\t \n",
      "Topic: 0.014*\"elect\" + 0.008*\"labor\" + 0.007*\"liber\" + 0.006*\"parti\" + 0.006*\"say\" + 0.006*\"senat\" + 0.006*\"govern\" + 0.006*\"andrew\" + 0.005*\"christma\" + 0.005*\"jam\"\n",
      "\n",
      "Score: 0.012503585778176785\t \n",
      "Topic: 0.017*\"countri\" + 0.013*\"donald\" + 0.012*\"hour\" + 0.009*\"stori\" + 0.007*\"wednesday\" + 0.007*\"care\" + 0.006*\"climat\" + 0.006*\"chang\" + 0.005*\"farmer\" + 0.005*\"water\"\n",
      "\n",
      "Score: 0.012503149919211864\t \n",
      "Topic: 0.026*\"trump\" + 0.020*\"news\" + 0.017*\"market\" + 0.016*\"rural\" + 0.008*\"share\" + 0.007*\"nation\" + 0.007*\"price\" + 0.007*\"rise\" + 0.007*\"dollar\" + 0.007*\"australian\"\n",
      "\n",
      "Score: 0.012503040954470634\t \n",
      "Topic: 0.010*\"hobart\" + 0.009*\"scott\" + 0.008*\"sexual\" + 0.008*\"street\" + 0.007*\"island\" + 0.006*\"tree\" + 0.006*\"islam\" + 0.006*\"right\" + 0.006*\"human\" + 0.006*\"music\"\n",
      "\n",
      "Score: 0.012502933852374554\t \n",
      "Topic: 0.015*\"interview\" + 0.008*\"morrison\" + 0.006*\"novemb\" + 0.006*\"marriag\" + 0.006*\"syria\" + 0.006*\"extend\" + 0.006*\"kill\" + 0.005*\"facebook\" + 0.005*\"hong\" + 0.005*\"kong\"\n",
      "\n",
      "Score: 0.01250247098505497\t \n",
      "Topic: 0.019*\"live\" + 0.008*\"cattl\" + 0.008*\"week\" + 0.008*\"search\" + 0.007*\"septemb\" + 0.007*\"miss\" + 0.007*\"june\" + 0.006*\"plane\" + 0.006*\"cancer\" + 0.006*\"anim\"\n",
      "\n",
      "Score: 0.01250229962170124\t \n",
      "Topic: 0.018*\"south\" + 0.015*\"north\" + 0.011*\"west\" + 0.010*\"coast\" + 0.009*\"queensland\" + 0.009*\"weather\" + 0.009*\"australia\" + 0.008*\"bushfir\" + 0.007*\"korea\" + 0.007*\"storm\"\n",
      "\n",
      "Score: 0.012502246536314487\t \n",
      "Topic: 0.017*\"charg\" + 0.017*\"polic\" + 0.016*\"murder\" + 0.012*\"woman\" + 0.011*\"court\" + 0.010*\"jail\" + 0.010*\"death\" + 0.010*\"crash\" + 0.010*\"alleg\" + 0.009*\"shoot\"\n",
      "\n",
      "Score: 0.012502211146056652\t \n",
      "Topic: 0.013*\"drum\" + 0.012*\"world\" + 0.011*\"final\" + 0.009*\"leagu\" + 0.008*\"tuesday\" + 0.008*\"friday\" + 0.008*\"cricket\" + 0.007*\"australia\" + 0.007*\"grandstand\" + 0.006*\"open\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.36694255471229553\t Topic: 0.009*\"health\" + 0.008*\"brisban\" + 0.008*\"rural\" + 0.007*\"tasmanian\" + 0.007*\"tasmania\"\n",
      "Score: 0.2000488042831421\t Topic: 0.026*\"polic\" + 0.015*\"charg\" + 0.013*\"court\" + 0.013*\"death\" + 0.012*\"murder\"\n",
      "Score: 0.19951216876506805\t Topic: 0.019*\"trump\" + 0.013*\"say\" + 0.012*\"govern\" + 0.010*\"chang\" + 0.008*\"school\"\n",
      "Score: 0.1994728296995163\t Topic: 0.015*\"queensland\" + 0.012*\"market\" + 0.012*\"donald\" + 0.011*\"news\" + 0.009*\"coast\"\n",
      "Score: 0.03402366116642952\t Topic: 0.030*\"australia\" + 0.024*\"australian\" + 0.016*\"elect\" + 0.014*\"world\" + 0.009*\"test\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
